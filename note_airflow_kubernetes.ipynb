{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discussion about deployment vs statefulset\n",
    "    # Deployment is a resource to deploy a stateless application, if using a PVC, all replicas will be using the same Volume and none of it will have its own state.\n",
    "    # Statefulsets is used for Stateful applications, each replica of the pod will have its own state, and will be using its own Volume.\n",
    "\n",
    "#Note about statefull vs stateless application in kubernetes:\n",
    "    # Stateless application is an application which do not depend on outside resource like database\n",
    "    # Statefull application is an application which depend on other outside resource like database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note about worker component to deploy worker:\n",
    "    #StatefulSet: is object which use to deploy stateful application\n",
    "    #Service: define how to access pod of Deployment or StatefulSet\n",
    "    #PodDisruptionBudget (pod disruption budget): define the minimum pod of Deployment or StatefulSet. Therefore, we can not delete by command if pod equal to minimum limit\n",
    "    #HorizontalPodAutoscaler: define how to increase number of pods\n",
    "\n",
    "#Note about horizontal vs vertical autoscaling: \n",
    "    # horizontal scaling means increase the number of pod.\n",
    "    # vertical scaling means increase resource for each pod.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note about StatefulSet important attributes:\n",
    "    # because each pod deploy with statefulset have it own volume. we have to define VolumeMount and volume or volumeClaimTemplates\n",
    "    # like Deployment, we have to set replica (how many pod do we want), template and container (what application we want to deploy)\n",
    "\n",
    "#Note about Service in worker conifg:\n",
    "    # service: define out how access pod. we have to define protocal (which protocal use to access pod), port (which port of service resource to access), selector (how to select pod which belong to that service)\n",
    "\n",
    "#Note about PodDisruptionBudget:\n",
    "    # definition: it define minimum of pod of a service. \n",
    "    # we have to define selector (how to select pods), minAvailable or maxUnavailable\n",
    "#Note about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note about Kind:\n",
    "    #definition: kind is a tools which allow us to run kubernetes with multiple nodes in your local computer using docker.\n",
    "\n",
    "#Note about kubectl:\n",
    "    #kubectl is a command line tools to control your kubernetes cluster\n",
    "\n",
    "#Note about helm:\n",
    "    #helm is a command line tools and software which allow us to define and deploy kubernetes objects which instructure as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the person we need to follow in term of airflow and kubernetes is Marc Lamberti. github profile: https://github.com/marclamberti\n",
    "# this note is follow this github repo: https://github.com/marclamberti/webinar-airflow-chart\n",
    "#let try to deploy kubernetes with local kubernetes cluster with multiple nodes (3 nodes (1 control + 2 worker))\n",
    "    #the first step is to create local kubernetes cluster with multiple node:\n",
    "    #define the cluster with kind and then deploy cluster with command: kind create cluster --name $cluster_name --config $config_file_path\n",
    "    #example: kind create cluster --name kind-cluster --config kind-cluster.yaml\n",
    "    #please read the config file for more information about how to config a cluster in kind\n",
    "    \n",
    "    #after create cluster, we check information about cluster with command: kubectl cluster-info, and kubectl get nodes -o wide\n",
    "\n",
    "    #after we have check cluster information let create namespace for kubernetes object management: kubectl create namespace $namespace\n",
    "    #check the newly created namespace with command: kubectl get namespace\n",
    "\n",
    "    #after create namespace for kubernetes object management. we add airflow official helm chart to our local repo with command: helm repo add [name] [url] [flags]\n",
    "    #example: helm repo add apache-airflow https://airflow.apache.org\n",
    "    #to get the latest version of apache-airflow helm chart use command: helm repo update\n",
    "    #to check that your helm chart is add to your local repo use command: helm search repo $search_key\n",
    "\n",
    "    #after we have add and update the helm chat and then check with command helm search. we will deploy the chart with helm command.\n",
    "    #deploy helm chart with command: helm install $release_name $chart_name --namespace $namespace --debug\n",
    "    #example: helm install airflow apache-airflow/airflow --namespace airflow --debug --timeout 10\n",
    "    #uninstall syntax: helm uninstall $release_name -n $namespace\n",
    "\n",
    "    #check to check your install or deployment use command: helm ls -n $namespace. \n",
    "    #helm ls will return deployment name, namespace and version. version allow us to quickly return to previous version if we have something wrong with the new version.\n",
    "\n",
    "    #check the status of pods with command: kubectl get pods -n $namespace\n",
    "\n",
    "    #check the log of pods use command: kubectl logs $pod_name -n $namespace\n",
    "\n",
    "    #you want to acess the airflow web service from local host use forwar-port command\n",
    "    #example: kubectl port-forward $resource $local_port:$service_port\n",
    "\n",
    "    #note that: each pod, service or other kubernetes resource is jut a container running in kubernetes cluster.\n",
    "    #port-forward command is to create a tunnel between local host port to the container port of the resource.\n",
    "\n",
    "    #from the begining, we use offical helm chart which is own by helm. we need to get config file to do the config\n",
    "    #the helm chart is design so that we have multiple chart components but only one central config  named values.yaml\n",
    "    #get config file with command: helm show values apache-airflow/airflow > values.yaml\n",
    "\n",
    "    #Note about helm show command. which allow us to show the content of the component of helm chart.\n",
    "    #helm show chart\n",
    "    #helm show values\n",
    "\n",
    "    #let discuss about values.yaml file, which is the config file of airflow with helm.\n",
    "    #airflowHome: is the home directory of airflow contaiern\n",
    "    #defaultAirflowRepository: docker repository where airflow image is pull from\n",
    "    #defaultAirflowTag: the tag which define which image to pull from repository.\n",
    "    #airflowVersion: is the version of airflow in helm\n",
    "\n",
    "    #after explore the values.yaml file, we will do the actual change the config\n",
    "    #change defaultAirflowTag and airflowVersion to 2.1\n",
    "    #add configMap to values.yaml.\n",
    "    #define where to use configMap in config file with extraEnvFrom.\n",
    "    \n",
    "    #after define configMap we have to apply to kubernetes so that kubernets have that object.\n",
    "    #command to apply a resource defintion: kubectl apply -f variables.yaml\n",
    "\n",
    "    #after finish the config we update helm deployment with command: helm upgrade --install\n",
    "    #example: helm upgrade --install airflow apache-airflow/airflow --debug -n airflow -f values.yaml\n",
    "\n",
    "    #after we have succesful update your deployment then check at the front-end. and then check with pod container\n",
    "    #to go to the specific pod use command: kubectl exec --stdin --tty $pod_name -n $namespace -- bin/bash . kubectl exec --stdin --tty airflow-worker-0 -n airflow -- /bin/bash\n",
    "    #this is to check the config of configMap with environment variable\n",
    "\n",
    "    #the next requirements is install python packages. the best approach is build custom image with dependencies and then use that image in kubernetes.\n",
    "    #build image with command: docker build -t $image_name:tag $dockerfile_path\n",
    "    #after build image, we need to load or register this image to kubernetes cluster in this case we use kind\n",
    "    #command to load image to kind kubernetes cluster: kind load docker-image $image_name --name $cluster_name. \n",
    "    # example: kind load docker-image airflow-custom:1.0.0 --name kind-cluster\n",
    "\n",
    "    #after solve the problem with install python package by build custom image. we will setup remote dag\n",
    "    #the idea is that airflow will communicate with git repository using ssh private and public key.\n",
    "    #step one: create your ssh private and public key with command: ssh key-gen\n",
    "    #step two: to the your repository > setting > deployment key > add your public key to repok\n",
    "    #step thre: enable gitSync and do the config with repo and subPath (the dag directory relative to root of repo). remember to set correct ssh address format\n",
    "    #step four: create secrete in kubernete with command: kubectl create secret generic airflow-ssh-git-secret --from-file=gitSshKey=$path_to_private_key -n $namespace\n",
    "    #example: kubectl create secret generic airflow-ssh-git-secret --from-file=gitSshKey=/root/.ssh/airflow -n airflow\n",
    "    #step five: after create kubernete secret, we just config in values.yaml at sshKeySecret property\n",
    "    #step six: update again with new config values.yaml with command: helm upgrade --install airflow apache-airflow/airflow -f values.yaml -n airflow --debug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note about build custom airflow image with Dockerfile\n",
    "#in this file we use command FROM, COPY, RUN\n",
    "    #FROM $repository:tag\n",
    "    #COPY $host_file $container_address\n",
    "    #RUN command\n",
    "#Note: . character means current direct, .. character means parent directory. In the context of container . character mean the home directory of container\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
